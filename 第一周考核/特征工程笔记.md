## 数据预处理

---

### 插值法填充

#### 1.多项式插值

> 已知 $n+1$ 个互异的点 $P_1:(x_1,y_1)，P_2:(x_2,y_2)，...，P_{n+1}:(x_{n+1},y_{n+1})$
> 可以求得经过这n+1个点，最高次不超过n的多项式:$Y=a_0+a_1X+a_2X^2+...+a_nX^n$
> 其中计算系数A的公式如下：
>
> $A=(a_0,a_1,...,a_n)^T=X^{−1}Y$
>
> (1)其中$X，Y$形式如下，求待定系数A：
>
>$$
X = \left(\begin{matrix}
   1 & x_1 & x_1^2 & \cdots & x_1^n\\
   1 & x_2 & x_2^2 & \cdots & x_2^n\\
   \cdot & \cdot & \cdot & \cdot & \cdot \\
   1 & x_{n+1} & x_{n+1}^2 & \cdots &x_{n+1}^n
   \end{matrix}\right),
   Y = \left(\begin{matrix}
  y_1 \\ y_2 \\ \cdot \\ y_{n+1}
  \end{matrix}\right)
>$$
>
> (2)进行插值的公式，$Y=AX$
>

```python
import numpy as np
def Polynomial(x, y, test_x):
    test_x 的值一般是在缺失值的前几个或者后几个值当中，挑出一个作为参考值，
    将其值代入到插值模型之中，学习出一个值作为缺失值的填充值
    # 求待定系数
    array_x = np.array(x)  # 向量化
    array_y = np.array(y)
    n, X = len(x), []
    for i in range(n):  # 形成 X 矩阵
        l = array_x ** i
        X.append(l)
    X = np.array(X).T
    A = np.dot(np.linalg.inv(X), array_y)  # 根据公式求待定系数 A
    
    # 缺失值插值
    xx = []
    for j in range(n):
        k = test_x ** j
        xx.append(k)
    xx=np.array(xx)
    return np.dot(xx, A)
    
x, y, test_x = [1, 2, 3, 4], [1, 5, 2, 6], 3.5
Polynomial(x, y, test_x)
```

> 输出: **2.250000000000057**

#### 2.lagrange插值

> **理论公式及推导**
> 已知n+1个互异的点 $P_1:(x_1,y_1)，P_2:(x_2,y_2)，...，P_{n+1}:(x_{n+1},y_{n+1})$,令
>
>$$
l_i(x) = \prod_{(j\neq 0)(j=1)}^{n+1}\frac{x-x_i}{x_i-x_j}，（1）
>$$
>
>作为插值基底，则Lagrange值
>
>$$
L_i(x)=\sum_{i=1}^{n+1}\cdot l_i(x)\cdot y_i，（2）
>$$
```python
def Lagrange(x, y, test_x):
    '''
    所谓的插值法，就是在X范围区间中挑选一个或者自定义一个数值，
    然后代进去插值公式当中，求出数值作为缺失值的数据。
    '''
    n = len(x)
    L = 0
    for i in range(n):
        # 计算公式 1
        li = 1
        for j in range(n):
            if j != i:
                li *= (test_x-x[j])/(x[i]-x[j])
        # 计算公式 2
        L += li * y[i]
    return L
        
Lagrange(x, y, test_x) 
```

> 输出：**2.25**

---

### 无量纲化
#### 1.极差标准化（Min-nax）

> Min-max区间缩放法(**极差标准化**)
>
>$$
\tilde{X}=\frac{x_i-x_{min}}{x_{max}-x_{min}}
>$$

#### 2.极大值标准化(Max-abs)

> Max-abs (**极大值标准化**):
>
>$$
\tilde{X_i}=\frac{\mid x_i \mid}{\mid x_{max} \mid}
>$$

#### 3.标准差标准化(z-score)

> z-score 标准化(标准差标准化)为类似正态分布，均值为0，标准差为1
>
>$$
\tilde{X_i}=\frac{x_i-\mu}{\sigma}.
>$$

#### 4.归一化——总和标准化

> 归一化(总和标准化)
>
>$$
\tilde{X_i}=\frac{x_i}{\sum_{j=1}^n\mid x_j \mid}
>$$

#### 5.非线性归一化

> 非线性归一化：不服从正态分布的数据，使范围为R的数据映射到区间(0,1)内，作一个非线性映射。
> 
> **sigmoid函数**
>$$
>y=\frac{1}{1+e^{-x}}
>$$
**arctan函数**
>$$
>y=arctanx
>$$
**tanh函数**
>$$
>y=tanhx=\frac{e^x-e^{-x}}{e^x+e^{-x}}
>$$

---

### 连续变量离散化(粗度)

> 连续变量离散化又可以归纳为粗细度调整的问题。
>
> 对数据进行粗粒度、细粒度划分：
>
>- 粗粒度划分(连续数据离散化)：二值化或离散化或分桶法
>- 细粒度划分：将段落或句子细分具体到一个词语或者字

**离散化的通用流程**如下：

>（1）对此特征进行排序。
	（2）选择某个点作为候选断点，用所选取的具体的离散化方法的尺度进行衡量此候选断点是否满足要求。
（3）若候选断点满足离散化的衡量尺度，则对数据集进行分裂或合并，再选择下一个候选断点，重复步骤（2）（3）。
（4）当离散算法存在停止准则时，如果满足停止准则，则不再进行离散化过程，从而得到最终的离散结果。

#### 1.特征二值化

> 设定一个划分的阈值
> 赋值的值可以根据实际需要来自定义设定
>
>$$
X=\begin{cases}
1,&X>threshold(阈值)\\
0,&X\le threshold(阈值)
\end{cases}
>$$

#### 2.无监督离散化

> (一)分箱法
>
> > 分箱法又分为等宽(宽度)分箱法和等频(频数)分箱法，它们的概念介绍如下：
> >
> >1. 等宽分箱法(基于属性/特征值大小区间来划分)
> >2. 等频分箱法(基于样本数量区间来划分)
>
> (二)聚类划分
>
> ​	聚类划分：使用聚类算法将数据聚成几类，每一个类为一个划分。
>
> ​	理论公式及推导
>
> ​	设有一维特征 $X=[x_1,x_2,\cdots ,x_n]$，理论假设如下：
>
> ​	(1)等宽分箱法：
> >假设 X 的最小值 $x_{min}=0$，最大值 $x_{max}=80$，可将 X 划分成 4 等份，其区间划分为(0, 20), (21, 40), (41, 60), (61, 80)，每一个区间对应着一个离散值。
>
> ​	(2)等频分箱法：
> ​		按频数划分
>
> ​	(3)聚类划分：
> ​		使用 K-Means 聚类算法进行无监督划分为 k 等份。
>
> 缺点
> 	无监督的方法的缺陷在于它对分布不均匀的数据不适用，对异常点比较敏感。
---

### 类别数据处理

将类别型数据转换成数值型数据

#### 1.序号编码(Ordinal Encoding)

>处理类别间具有大小关系的数据，比如成绩(高中低)
>
>假设有类别数据X=(x1,x2,...,xn),则每一个唯一的类别型元素对应着一个
>
>数字,即键值对dict={key1:0, key2:1,..., keyk:k}
#### 2.独热编码(One-hot Encoding)
> 通常用于处理类别间不具有大小关系的特征，比如血型(A型血、B型血、AB型血、O型血)，
> 独热编码会把血型变成一个稀疏向量，A型血表示为(1,0,0,0)，B型血表示为(0,1,0,0)，
> AB型血表示为(0,0,1,0)，O型血表示为(0,0,0,1)

#### 3.二进制编码(Binary Encoding)

> 利用二进制进行哈希映射

---

## 特征构造

**1. 概念及工作原理**
	概念：产生衍生变量，生成有商业意义的新变量(新特征)

**2. 别称**
	特征交叉、特征组合、数据变换

---

### 1.特征设计原理

> 新特征设计应与目标高度相关，要考虑的问题：
>
>1. 这个特征是否对目标有实际意义？
>2. 如果有用，这个特征重要性如何？
>3. 这个特征的信息是否在其他特征上体现过？
>

---

### 2.特征构造常用方法

#### 2.1.统计值构造法

> 单变量：
> 如果某个特征与目标高度相关，那么可以根据具体的情况取这个特征的统计值作为新的特征。
>多变量：
> 如果特征与特征之间存在交互影响时，那么可以聚合分组两个或多个变量之后，再以统计值构造出新的特征。

#### 2.2.函数变换法

> **概念及工作原理**
> 	简单常用的函数变换法(一般针对于连续数据)：
> 	(1)平方(小数值—>大数值)
> 	(2)开平方(大数值—>小数值)
> 	(3)指数
> 	(4)对数
> 	(5)差分

## 特征选择

### 1.特征选择概述
>
>**概念及工作原理**
>
>1. 从两个方面考虑来选择特征，如下：
>     
>     > (1)特征是否发散
>    > 	如果一个特征不发散，这个特征对于样本的区分并无用处。
>     > (2)特征与目标的相关性
>     > 	与目标相关性高的特征，优先选择。
>     > 	区别：特征与特征之间相关性高的，只取一个。

### 2.Filter 过滤法

#### 	**2.1.方差选择法**
>变量的方差越大，这个变量对模型的贡献和作用会更明显，因此要保留方差较大的变量，反之，要剔除掉无意义的特征。
#### 	2.2.相关系数法
>
> 第一种方法：计算特征与特征的相关系数
>
> **理论公式及推导**
> 假设X=(x1,x2,...,xn)，其中x1，x2...是列向量，即x1代表一个特征，公式推导如下：
>
>$$
corr(X1,X2)=\frac{cov(X1,X2)}{\sigma_{x1}\sigma_{x2}}=\frac{\sum_{i=1}^n(x1_i-\mu_{x1})(x2_i-\mu_{x2})}  
{\sqrt{\sum_{i=1}^n(x1_i-\mu_{x1})^2}\sqrt{\sum_{i=1}^n(x2_i-\mu_{x2})^2}}，其中\mu表示各个特征的平均值
>$$
>
> 第二种方法：计算特征与目标的相关系数以及P值
> 
> **原理依据**
>scipy.stats.pearsonr(x, y)
> 输出:(r, p)
>r:相关系数(-1，1)之间
> p:相关系数显著性
> 
> **缺点**
> 如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0

```python
# 相关系数--特征与目标变量
def corr_selector(df):
    X, y = df.iloc(:, :4), df.iloc(:, 4)
    cor_list = ()
    for i in X.columns.tolist():
        cor = np.corrcoef(X(i), y)(0, 1)py
        cor_list.append(cor)
    print(X.columns.tolist())
    print(cor_list)
    return cor_list
corr_selector(df)
```

#### 2.3.卡方检验
> **工作原理**
>  卡方检验是检验定性自变量对定性因变量的相关性，求出卡方值，然后根据卡方值匹配出其所对应的概率是否足以推翻原假设H0，如果能推翻H0，就启用备用假设H1。
>  
> **理论公式及推导**
> 假设检验：
> 假如提出原假设H0：化妆与性别没有关系；
> 备用假设H1：化妆与性别有显著关系。
> 
> 卡方值(chi-square-value)计算公式
> 
>$$
>\chi^2=\sum\frac{(A-E)^2}{E}=\sum_{i=1}^k\frac{(A_i-nP_i)^2}{nP_i}
$$
>
>其中，$A_i$ 为 $i$ 水平的观察(实际)频数，$E_i$ 为 $i$ 水平的期望(理论)频数，n 为总频数，$P_i$ 为 $i$ 水平的期望频率。
> $i$ 水平的期望频数 $E_i$ 等于总频数 $n*i$ 水平的期望概率 $P_i$，$k$ 为单元格数(行数*列数)。
> 
>如何判定两个定性变量的卡方值在什么区间可以证明假设成不成立呢？
> 计算步骤如下：
> 
>1. 计算卡方值 $χ_2$ 卡方值
> 2. 计算自由度(df=(行数-1)*(列数-1))
> 3. 置信度 (根据卡方值结合表格和自由度查询而得到的置信度大小)
> 
#### 2.4.互信息法
> **工作原理**
> 评价定性自变量对定性应变量的相关性
>
> 在处理分类问题提取特征的时候就可以用互信息来衡量某个特征和特定类别的相关性，如果信息量越大，那么特征和这个类别的相关性越大。
>
> **应用场景及意义**
> 应用场景：因此非常适合于文本分类的特征和类别的配准工作
>
> **理论公式及推导**
>
> 1. 互信息公式：
>
> $$
> \begin{align}
> & MI(X,Y)=
> \sum_{i=1}^{\mid X \mid}\sum_{j=1}^{\mid Y \mid}P(x_i,y_i)\log_2\frac{P(x_i,y_j)}{P(x_i)P(y_j)} \\
> & P(x_i,y_j)=\frac{|X_i \cap Y_j|}{N}，\\ 
> & P(x_i)=\frac{X_i}{N}，P(Y_j)=\frac{Y_j}{N}，N是总样本数
> \end{align}
> $$
>
> 2. 标准互信息公式：
>
> $$
> NMI(X,Y)=\frac{2MI(X,Y)}{H(X)+H(Y)}
> $$
>
> > 其中信息熵的公式
>
> $$
> H(X)=-\sum_{i=1}^XP(x_i)log_2(P(x_i))，\\ 
> H(Y)=-\sum_{j=1}^YP(y_j)log_2(P(y_j))
> $$

```python
import math
import numpy as np
from sklearn import metrics
def NMI(A,B):
    #样本点数
    total, A_ids, B_ids = len(A), set(A), set(B)
    #互信息计算
    MI, eps = 0, 1.4e-45
    for idA in A_ids:
        for idB in B_ids:
            idAOccur = np.where(A==idA)
            idBOccur = np.where(B==idB)
            idABOccur = np.intersect1d(idAOccur,idBOccur)
            px = 1.0*len(idAOccur[0])/total
            py = 1.0*len(idBOccur[0])/total
            pxy = 1.0*len(idABOccur)/total
            MI = MI + pxy*math.log(pxy/(px*py)+eps,2)
    # 标准化互信息
    Hx, Hy = 0, 0
    for idA in A_ids:
        Hx -= px*math.log(idAOccurCount/total+eps,2)
    for idB in B_ids:
        Hy -= py*math.log(idBOccurCount/total+eps,2)
    MIhat = 2.0*MI/(Hx+Hy)
    return MIhat
A = np.array([1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3])
B = np.array([1,2,1,1,1,1,1,2,2,2,2,3,1,1,3,3,3])
print(NMI(A,B))
print(metrics.normalized_mutual_info_score(A,B)) #sklearn结果
```



