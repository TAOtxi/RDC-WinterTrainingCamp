[TOC]
## 数据预处理

---

### 插值法填充

#### 1.多项式插值

> 已知 $n+1$ 个互异的点 $P_1:(x_1,y_1)，P_2:(x_2,y_2)，...，P_{n+1}:(x_{n+1},y_{n+1})$
> 可以求得经过这n+1个点，最高次不超过n的多项式:$Y=a_0+a_1X+a_2X^2+...+a_nX^n$
> 其中计算系数A的公式如下：
>
> $A=[a_0,a_1,...,a_n]^T=X^{−1}Y$，其中$X^{−1}$是$X$的逆矩阵
>
> (1)其中$X，Y$形式如下，求待定系数A：

$$
X = \left[\begin{matrix}
   1 & x_1 & x_1^2 & \cdots & x_1^n\\
   1 & x_2 & x_2^2 & \cdots & x_2^n\\
   \cdot & \cdot & \cdot & \cdot & \cdot \\
   1 & x_{n+1} & x_{n+1}^2 & \cdots &x_{n+1}^n
   \end{matrix}\right],
   Y = \left[\begin{matrix}
  y_1 \\ y_2 \\ \cdot \\ y_{n+1}
  \end{matrix}\right]
$$

> (2)进行插值的公式，$Y=AX$
>

```python
import numpy as np

def Polynomial(x, y, test_x):
    '''
    test_x 的值一般是在缺失值的前几个或者后几个值当中，挑出一个作为参考值，
    将其值代入到插值模型之中，学习出一个值作为缺失值的填充值
    '''
    # 求待定系数
    array_x = np.array(x)  # 向量化
    array_y = np.array(y)
    n, X = len(x), []
    for i in range(n):  # 形成 X 矩阵
        l = array_x ** i
        X.append(l)
    X = np.array(X).T
    A = np.dot(np.linalg.inv(X), array_y)  # 根据公式求待定系数 A
    
    # 缺失值插值
    xx = []
    for j in range(n):
        k = test_x ** j
        xx.append(k)
    xx=np.array(xx)
    return np.dot(xx, A)
    
x, y, test_x = [1, 2, 3, 4], [1, 5, 2, 6], 3.5
Polynomial(x, y, test_x)
```

> 输出: **2.250000000000057**

#### 2.lagrange插值

> **理论公式及推导**
> 已知n+1个互异的点 $P_1:(x_1,y_1)，P_2:(x_2,y_2)，...，P_{n+1}:(x_{n+1},y_{n+1})$,令

$$
l_i(x) = \prod_{(j\neq 0)(j=1)}^{n+1}\frac{x-x_i}{x_i-x_j}，（1）
$$

>作为插值基底，则Lagrange值

$$
L_i(x)=\sum_{i=1}^{n+1}\cdot l_i(x)\cdot y_i，（2）
$$
```python
def Lagrange(x, y, test_x):
    '''
    所谓的插值法，就是在X范围区间中挑选一个或者自定义一个数值，
    然后代进去插值公式当中，求出数值作为缺失值的数据。
    '''
    n = len(x)
    L = 0
    for i in range(n):
        # 计算公式 1
        li = 1
        for j in range(n):
            if j != i:
                li *= (test_x-x[j])/(x[i]-x[j])
        # 计算公式 2
        L += li * y[i]
    return L
        
Lagrange(x, y, test_x) 
```

> 输出：**2.25**

---

### 无量纲化
#### 1.极差标准化（Min-nax）

> Min-max区间缩放法(**极差标准化**)，将数值缩放到[0, 1]区间

$$
\tilde{X}=\frac{x_i-x_{min}}{x_{max}-x_{min}}
$$

#### 2.极大值标准化(Max-abs)

> Max-abs (**极大值标准化**)，标准化之后的每一维特征最大要素为1，其余要素均小于1，理论公式如下：

$$
\tilde{X_i}=\frac{\mid x_i \mid}{\mid x_{max} \mid}
$$

#### 3.标准差标准化(z-score)

> z-score 标准化(**标准差标准化**)为类似正态分布，均值为0，标准差为1

$$
\tilde{X_i}=\frac{x_i-\mu}{\sigma}.
$$

#### 4.归一化——总和标准化

> 归一化(总和标准化)，常用到权重值来表示其重要性

$$
\tilde{X_i}=\frac{x_i}{\sum_{j=1}^n\mid x_j \mid}
$$

#### 5.非线性归一化

> 非线性归一化：对于所属范围未知或者所属范围是全体实数，同时不服从正态分布的数据，
> 对其作Min-max标准化、z-score标准化或者归一化都是不合理的。
> 要使范围为R的数据映射到区间[0,1]内，需要作一个非线性映射。而常用的有sigmoid函数、arctan函数和tanh函数。

>**sigmoid函数**
$$
y=\frac{1}{1+e^{-x}}
$$
>**arctan函数**
$$
y=arctanx
$$
>**tanh函数**
$$
y=tanhx=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

---

### 连续变量离散化(粗度)

> 连续变量离散化又可以归纳为粗细度调整的问题。
>
> 对数据进行粗粒度、细粒度划分：
>
>- 粗粒度划分(连续数据离散化)：将年龄段0~100岁的连续数据进行粗粒度处理，也可称为二值化或离散化或分桶法
>- 细粒度划分：将段落或句子细分具体到一个词语或者字

**离散化的通用流程**如下：

>（1）对此特征进行排序。
	（2）选择某个点作为候选断点，用所选取的具体的离散化方法的尺度进行衡量此候选断点是否满足要求。
（3）若候选断点满足离散化的衡量尺度，则对数据集进行分裂或合并，再选择下一个候选断点，重复步骤（2）（3）。
（4）当离散算法存在停止准则时，如果满足停止准则，则不再进行离散化过程，从而得到最终的离散结果。

#### 1.特征二值化

> 设定一个划分的阈值
> 赋值的值可以根据实际需要来自定义设定

$$
X=\begin{cases}
1,&X>threshold(阈值)\\
0,&X\le threshold(阈值)
\end{cases}
$$

#### 2.无监督离散化

> (一)分箱法
> 
>> 分箱法又分为等宽(宽度)分箱法和等频(频数)分箱法，它们的概念介绍如下：
> >
> >1. 等宽分箱法(基于属性/特征值大小区间来划分)：按照相同宽度将数据分成几等份。
> >2. 等频分箱法(基于样本数量区间来划分)：将数据分成几等份，每等份数据里面的个数(数量/频数)是一样的。
> 
>(二)聚类划分
> 
>​	聚类划分：使用聚类算法将数据聚成几类，每一个类为一个划分。
> 
> ​	理论公式及推导
> 
> ​	设有一维特征 $X=[x_1,x_2,\cdots ,x_n]$，理论假设如下：
> 
> ​	(1)等宽分箱法：
> 
> > 假设 X 的最小值 $x_{min}=0$，最大值 $x_{max}=80$，那么按照等宽分箱法定义
> 可以将 X 划分成 4 等份，其区间划分为[0, 20], [21, 40], [41, 60], [61, 80]，每一个区间对应着一个离散值。
> >
> > 推广通用理论(请注意：为了方便计算，k 从 1 开始，而不是从 0 开始)：
> > 设 X 属性值的 $x_{min}=a，x_{max}=b$，将连续数据按照等宽法定义离散为 k 等份，则：
> > 离散值为
$$
value=[1,2,3,\cdots,k]
$$
> > 划分属性值宽度为
$$
width=(b-a)/k，其中b=a+k*width
$$
> > 那么划分区间为 k 等份，每个区间对应着一个离散值
$$
\left[\begin{matrix}
[a,a+width]\\
[a+width,a+2*width]\\
\cdots\\\cdots\\\cdots\\
[a+(k-1)*width,a+k*width]\end{matrix}\right]
\Rightarrow
\left[\begin{matrix}
value(1)\\
value(2)\\
\cdot\\\cdot\\\cdot\\
value(k)
\end{matrix}\right]
$$
>(2)等频分箱法：
>
>​	假设 X 的样本数量有 80 个，那么按照等频分箱法定义，可以划分为 4 等份，每 20 个样本划分为 1 等份。
>
>(3)聚类划分：
>
>​	使用 K-Means 聚类算法进行无监督划分为 k 等份。
>
>优缺点
>	无监督的方法的缺陷在于它对分布不均匀的数据不适用，对异常点比较敏感。
---

### 类别数据处理

很多算法模型不能直接处理字符串数据，因此需要将类别型数据转换成数值型数据

#### 1.序号编码(Ordinal Encoding)

>处理类别间具有大小关系的数据，比如成绩(高中低)
>
>假设有类别数据X=[x1,x2,...,xn],则序号编码思想如下：
>
>- (1)确定X中唯一值的个数K，将唯一值作为关键字，即
>
>   Key=[x1,x2,...,xk]
>
>- (2)生成k个数字作为键值，即
>
>   Value=[0,1,2,...,k]
>
>- (3)每一个唯一的类别型元素对应着一个数字,即键值对
>
>   dict={key1:0, key2:1,..., keyk:k}
>
>
#### 2.独热编码(One-hot Encoding)
> 通常用于处理类别间不具有大小关系的特征，比如血型(A型血、B型血、AB型血、O型血)，
> 独热编码会把血型变成一个稀疏向量，A型血表示为(1,0,0,0)，B型血表示为(0,1,0,0)，
> AB型血表示为(0,0,1,0)，O型血表示为(0,0,0,1)
> **提示**：
> (1)在独热编码下，特征向量只有某一维取值为1，其余值均为0，因此可以利用向量的稀疏来节省空间
> (2)如果类别型的唯一类别元素较多，可能会造成维度灾难，因此需要利用特征选择来降低维度。
>
> 假设有类别数据X=[x1,x2,...,xn],则独热编码思想如下：
>
> - (1)确定X中唯一值的个数K，将唯一值作为关键字，即
>
>   Key=[x1,x2,...,xk]
>
> - (2)生成k个数字为1的一维数组作为键值，即
>
>    Value=[1,1,1,...,k]
>
> - (3)每一个唯一的类别型元素对应着一个数字,即键值对
>
>    dict={key1:0, key2:1,..., keyk:k}
>
> - (4)创建一个空的数组v=V(n维 x k维)=np.zeros((n, k))
>
> - (5)将数值对应的那一维为1，其余为0，最后将V与原始数据合并即可
>

#### 3.二进制编码(Binary Encoding)

> 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后将类别ID对应的二进制编码作为结果。
> 以A、B、AB、O血型为例，表1.1是二进制编码的过程。
> A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为010；
> 以此类推可以得到AB型血和O型血的二进制表示。
> 可以看出，二进制编码本质上是利用二进制对ID进行哈希映射，
> 最终得到0/1特征向量，且维数少于独热编码，节省了存储空间。

---

## 特征构造

**1. 概念及工作原理**
	概念：产生衍生变量，生成有商业意义的新变量(新特征)

**2. 别称**
	特征交叉、特征组合、数据变换

---

### 1.特征设计原理

> 新特征设计应与目标高度相关，要考虑的问题：
>
>1. 这个特征是否对目标有实际意义？
>2. 如果有用，这个特征重要性如何？
>3. 这个特征的信息是否在其他特征上体现过？
>
>
>新构建特征验证其有效性要考虑的问题：
>
>1. 需要领域知识、直觉、行业经验以及数学知识综合性考量特征的有效性，防止胡乱构造没有意义的特征。
>2. 要反复与模型进行迭代验证其是否对模型有正向促进作用。
>3. 或者进行特征选择判定新构建特征的重要性来衡量其有效性。

---

### 2.特征构造常用方法

#### 2.1.统计值构造法

> 概念及工作原理
> 概念：指通过统计单个或者多个变量的统计值(max,min,count,mean)等而形成新的特征。
>
> 单变量：
> 如果某个特征与目标高度相关，那么可以根据具体的情况取这个特征的统计值作为新的特征。
> 多变
> 如果特征与特征之间存在交互影响时，那么可以聚合分组两个或多个变量之后，再以统计值构造出新的特征。

#### 2.2.函数变换法

> **概念及工作原理**
> 	简单常用的函数变换法(一般针对于连续数据)：
> 	(1)平方(小数值—>大数值)
> 	(2)开平方(大数值—>小数值)
> 	(3)指数
> 	(4)对数
> 	(5)差分

## 特征选择

### 1.特征选择概述
>
>**概念及工作原理**
>
>1. 从两个方面考虑来选择特征，如下：
>     
>     > (1)特征是否发散
>    > 	如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并	没有什么用。
>     > (2)特征与目标的相关性
>     > 	与目标相关性高的特征，应当优先选择。
>     > 	区别：特征与特征之间相关性高的，应当优先去除掉其中一个特征，因为它们是替代品。

### 2.Filter 过滤法

#### 	**2.1.方差选择法**
>变量的方差越大，这个变量对模型的贡献和作用会更明显，因此要保留方差较大的变量，反之，要剔除掉无意义的特征。
#### 	2.2.相关系数法
>
> 第一种方法：计算特征与特征的相关系数
>
> 工作原理
> 通过计算特征与特征之间的相关系数的大小，可判定两两特征之间的相关程度。
> 取值区间在[-1, 1]之间，取值关系如下：
> corr(x1,x2)相关系数值小于0表示负相关((这个变量下降，那个就会上升))，即x1与x2是互补特征
> corr(x1,x2)相关系数值等于0表示无相关
> corr(x1,x2)相关系数值大于0表示正相关，即x1与x2是替代特征
> 原理实现：取相关系数值的绝对值，然后把corr值大于90%~95%的两两特征中的某一个特征剔除。
>
> 如果两个特征是完全线性相关的，这个时候我们只需要保留其中一个即可。
> 因为第二个特征包含的信息完全被第一个特征所包含。
> 此时，如果两个特征同时都保留的话，模型的性能很大情况会出现下降的情况
>
> **理论公式及推导**
> 假设X=[x1,x2,...,xn]，其中x1，x2...是列向量，即x1代表一个特征，公式推导如下：

$$
corr(X1,X2)=\frac{cov(X1,X2)}{\sigma_{x1}\sigma_{x2}}=\frac{\sum_{i=1}^n(x1_i-\mu_{x1})(x2_i-\mu_{x2})}  
{\sqrt{\sum_{i=1}^n(x1_i-\mu_{x1})^2}\sqrt{\sum_{i=1}^n(x2_i-\mu_{x2})^2}}，其中\mu表示各个特征的平均值
$$

> 第二种方法：计算特征与目标的相关系数以及P值
> 
> **原理依据**
>scipy.stats.pearsonr(x, y)
> 输出:(r, p)
>r:相关系数[-1，1]之间
> p:相关系数显著性
> 
> 相关性的强度确实是用相关系数的大小来衡量的，但相关大小的评价要以相关系数显著性的评价为前提
> 因此，要先检验相关系数的显著性，如果显著，证明相关系数有统计学意义，下一步再来看相关系数大小；
> 如果相关系数没有统计学意义，那意味着你研究求得的相关系数也许是抽样误差或者测量误差造成的，再进行一次研究结果可
>能就大不一样，此时讨论相关性强弱的意义就大大减弱了。
> 
> 原理实现：先计算各个特征对目标值的相关系数以及相关系数的P值
> 
> **缺点**
>Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。
> 如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0

```python
# 相关系数--特征与目标变量
def corr_selector(df):
    X, y = df.iloc[:, :4], df.iloc[:, 4]
    cor_list = []
    for i in X.columns.tolist():
        cor = np.corrcoef(X[i], y)[0, 1]py
        cor_list.append(cor)
    print(X.columns.tolist())
    print(cor_list)
    return cor_list
corr_selector(df)
```

#### 2.3.卡方检验
> **工作原理**
>  卡方检验是检验定性自变量对定性因变量的相关性，求出卡方值，然后根据卡方值
>  匹配出其所对应的概率是否足以推翻原假设H0，如果能推翻H0，就启用备用假设H1。
> 
> **理论公式及推导**
> 假设检验：
> 假如提出原假设H0：化妆与性别没有关系；
> 备用假设H1：化妆与性别有显著关系。
> 
> 卡方值(chi-square-value)计算公式

$$
\chi^2=\sum\frac{(A-E)^2}{E}=\sum_{i=1}^k\frac{(A_i-nP_i)^2}{nP_i}
$$

> 其中，$A_i$ 为 $i$ 水平的观察(实际)频数，$E_i$ 为 $i$ 水平的期望(理论)频数，n 为总频数，$P_i$ 为 $i$ 水平的期望频率。
> $i$ 水平的期望频数 $E_i$ 等于总频数 $n*i$ 水平的期望概率 $P_i$，$k$ 为单元格数(行数*列数)。
>
> 如何判定两个定性变量的卡方值在什么区间可以证明假设成不成立呢？
> 计算步骤如下：
>
> 1. 计算卡方值 $χ_2$ 卡方值
> 2. 计算自由度(df=(行数-1)*(列数-1))
> 3. 置信度 (根据卡方值结合表格和自由度查询而得到的置信度大小)
>
> **优点**
> 可以很好地筛选出与定性应变量有显著相关的定性自变量。
>
> **应用场景及意义**
> 应用场景：适用于分类问题的分类变量
#### 2.4.互信息法
> 原理及实现参考资料-[互信息原理及实现](https://blog.csdn.net/DreamHome_S/article/details/78379635)
> **工作原理**
> 评价定性自变量对定性应变量的相关性
> 
> 在处理分类问题提取特征的时候就可以用互信息来衡量某个特征和特定类别的相关性，
> 如果信息量越大，那么特征和这个类别的相关性越大。反之也是成立的。
> 
> **应用场景及意义**
> 应用场景：因此非常适合于文本分类的特征和类别的配准工作
> 
> **理论公式及推导**
> 
> 1. 互信息公式：

$$
MI(X,Y)=\sum_{i=1}^X\sum_{j=1}^YP(x_i,y_i)\log_2\frac{P(x_i,y_j)}{P(x_i)P(y_j)}
$$

> > 其中

$$
P(x_i,y_j)=\frac{|X_i \cap Y_j|}{N}，P(x_i)=\frac{X_i}{N}，P(Y_j)=\frac{Y_j}{N}其中N是总样本数
$$

> 2. 标准互信息公式：

$$
NMI(X,Y)=\frac{2MI(X,Y)}{H(X)+H(Y)}
$$

> > 其中信息熵的公式

$$
H(X)=-\sum_{i=1}^XP(x_i)log_2(P(x_i))，而H(Y)=-\sum_{j=1}^YP(y_j)log_2(P(y_j))
$$
