# 机器学习


***
***
# 线性回归

***
| 梯度下降                 | 正规方程                 |
| ------------------------ | ------------------------ |
| 较好适用特征数量大的情况 | 特征数量$n\leq10000$最好 |
| 适用各种模型             | 只适用线性模型           |

***

## 梯度下降法
### 批量梯度下降

$$
\begin{align}
设回归方程 &:h(x) = \theta_0 + \theta_1 \cdot x \\
则代价函数 &:J(\theta_0，\theta_1) = \frac{1}{2m}\sum_{i=1}^m (h(x_i) - y_i)^2 \\
梯度下降法 &:\theta_i = \theta_i - \alpha \frac{\partial}{\partial\theta_i} J(\theta_0，\theta_1)，其中\alpha为学习效率 \\
\end{align}
$$

$$
\begin{align}
矩阵形式: &\\
h_\theta(X) &= X\theta \\
J(\theta) &= \frac{1}{2}(X\theta - Y)^T(X\theta - Y) \\
\theta &= \theta - \alpha\frac{\partial}{\partial\theta}J(\theta) \\
&= \theta - \alpha X^T(X\theta - Y)
\end{align}
$$

### 随机梯度下降

每次随机采用一个样本来迭代

### 小批量梯度下降

对于 $m$ 个样本，随机采用 $x$ 个样本来迭代。 $x$ 大小根据样本数据调整

---

## 正规方程推导[^1]

$$
\begin{align}
J(\theta) &= \frac{1}{2}(X\theta - y)^T (X\theta - y) \\
&= \frac{1}{2}(\theta^T X^T - y^T)(X\theta - y) \\
&= \frac{1}{2}(\theta^T X^T X\theta - \theta^T X^Ty -y^TX\theta + y^Ty) \\\\
\frac{\partial J(\theta)}{\partial\theta} &= \frac{1}{2}(2X^TX\theta - X^Ty -(y^TX)^T - 0) \\
&= X^TX\theta - X^Ty \\
&= 0 \\\\
\therefore \theta &= (X^TX)^{-1}X^Ty
\end{align}
$$
## 正则化
### Ridge回归（L2[^2]正则化）

$$
\begin{align}
J(\theta) &= \frac{1}{2}(X\theta - Y)^T(X\theta - Y) + \frac{1}{2}\alpha\mid\mid \theta \mid\mid_2^2 \\
\theta &= (X^TX + \alpha E)^{-1}X^TY
\end{align}
$$

### Lasso回归（L1[^3]正则化）

$$
\begin{align}
J(\theta) &= \frac{1}{2n}(X\theta - Y)^T(X\theta - Y) + \alpha\mid\mid \theta \mid\mid_1 ，n为样本个数
\end{align}
$$

#### 坐标轴下降法

坐标轴下降法在每次迭代中在当前点处沿一个坐标方向进行一维搜索 ，固定其他的坐标方向，找到一个函数的局部极小值。




***
# 分类算法

## Sigmoid函数

$$
\begin{align}
h(x)&=\frac1{1+e^{-x}} \\
h'(x)&=h(x)\cdot(1-h(x))
\end{align}
$$

## Logistic回归损失函数

$$
\begin{align}
似然函数：L(\theta) &= \prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\\\\
损失函数：J(\theta) &= -lnL(\theta) \\
&= -\sum\limits_{i=1}^{m}[y^{(i)}ln(h_{\theta}(x^{(i)}))+ (1-y^{(i)})ln(1-h_{\theta}(x^{(i)}))]\\\\
损失函数矩阵表达：J(\theta)&=-Y^TlnH(X\theta) - (E-Y)^T ln(E-H(X\theta)) \\\\
\frac{\partial}{\partial\theta}J(\theta) &= -[(E-H(X\theta))\circ Y-H(X\theta)\circ (E-Y)]^TX\\
&=(H(X\theta)-Y)^TX\\\\
&其中E为全1列向量
\end{align}
$$



***





[^1]: 矩阵需要求导

+ [矩阵求导本质](https://zhuanlan.zhihu.com/p/263777564)
+ [矩阵求导公式的数学推导](https://zhuanlan.zhihu.com/p/273729929)
>一些公式：
>$$
>\begin{align}
>\frac{\partial(x^Ta)}{\partial x} &= \frac{\partial(a^T x)}{\partial x} = a \tag{1}，其中a_{n \times 1}\\
>\frac{\partial(x^Tx)}{\partial x} &= 2x \tag{2}\\
>\frac{\partial(x^TAx)}{\partial x} &= Ax + A^Tx ，其中A_{n \times n} \tag{3} \\
>\frac{\partial(a^Txx^Tb)}{\partial x} &= ab^Tx + ba^Tx，其中a_{n \times 1}，b_{n \times 1} \tag{4} \\
>\frac{\partial(a^TXb)}{\partial X} &= ab^T ，其中a_{m \times 1},b_{n \times 1} \tag{5}\\
>\frac{\partial(a^TX^Tb)}{\partial X} &= ba^T，其中a_{n \times 1},b_{m \times 1} \tag{6} \\
>\frac{\partial(a^TXX^Tb)}{\partial X} &= ab^TX + ba^TX，其中a_{m \times 1},b_{m \times 1} \tag{7} \\
>\frac{\partial(a^TX^TXb)}{\partial X} &= Xba^T + Xab^T，其中a_{n \times 1},b_{n \times 1} \tag{8} \\
>\end{align}
>$$

[^2]:向量各元素的平方和的平方根
[^3]:向量中各个元素绝对值之和